{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练深度强化学习代理来玩“星际争霸 II (StarCraft II)”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欢迎来到本实验！开始之前，我们先来了解一下 Jupyter Notebook 上的一些指标。\n",
    "\n",
    "1.您可以使用浏览器打开 Jupyter Notebook 笔记本，不过具体内容则由在支持 AWS EC2 GPU 的实例上运行的交互式 iPython 内核进行流式传输。\n",
    "\n",
    "2.此笔记本由若干单元组成；单元中会包含您所能运行的代码，也能保存供您阅览的文本或图像。\n",
    "\n",
    "3.您可以通过单击菜单中的 ```Run```（运行）图标，或通过以下键盘快捷键 ```Shift-Enter```（运行并执行下一个）或 ```Ctrl-Enter```（运行并停留在当前单元）来执行代码单元。\n",
    "\n",
    "4.如要中止执行单元，请单击工具栏上的 ```Stop```（停止）按钮或前往 ```Kernel```（内核）菜单，并选择 ```Interrupt ```（中断）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我是一个标注单元 - 如果您运行我，我就会变为静态文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + 1  # i'm a code cell -- if you run me I'll perform some computations and show you their result below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习 (RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习是机器学习的一个子领域，在此领域中，学习代理并非使用标注好的数据集，而是通过与环境交互来构建自己的经验数据集。最初，代理的动作是随机的，但当其偶然发现良好的行为时，便会受到环境的奖励，而且此奖励信号允许代理更新自身参数，以便日后能够将其所获奖励最大化。\n",
    "\n",
    "<img src=\"images/RL.jpg\" width=\"95%\"></img>\n",
    "\n",
    "自从 Google 的 DeepMind 使用 RL 和深度学习 (DL) 训练 AI 代理，使其仅通过像素来学习玩游戏，并以此掌握 Atari 游戏技能后，RL 最近广受追捧。自那时起，DeepMind 就已使用深度强化学习 (DRL) 成功挑战了围棋游戏。\n",
    "\n",
    "RL 要面临的下一个未攻克的艰巨挑战则是“星际争霸 II (StarCraft II)”- <a href=\"https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/\">了解详情</a>！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#“星际争霸 II (StarCraft II)”\n",
    "“星际争霸 II (StarCraft II)”(SC2) 是一款由 Blizzard 开发的实时策略游戏，可供免费畅玩（<a href='https://starcraft2.com/en-us/'>游戏官方网站链接</a>）。\n",
    "单击图片观看概述视频。\n",
    "\n",
    "<a href='https://www.youtube.com/watch?v=yaqeZ9Snt4E'> <img src=\"images/sc2.jpg\"></img> </a>\n",
    "    \n",
    "实时策略游戏需要玩家具备许多技能，包括：战略思维、准确或快速执行、信息收集或隐藏以及经济资源管理。在本实验中，我们将训练神经网络代理来玩迷你游戏，从而剥离并捕获全局游戏的一些基本技能。\n",
    "\n",
    "<a href=\"\"><img src=\"https://storage.googleapis.com/deepmind-live-cms-alt/documents/mini-games.gif\" width=\"90%\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验组成部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本实验由以下部分组成：\n",
    "\n",
    "    \n",
    "<ul>\n",
    "<a href='#section1'>第 1 节</a>借助 DeepMind 的 Pysc2 API 以编程方式探索并掌握 SC2 的游戏技能\n",
    "    <br>&nbsp;<br>\n",
    "<a href='#section2'>第 2 节</a>了解如何使用奖励来塑造代理动作\n",
    "    <br>&nbsp;<br>\n",
    "<a href='#section3'>第 3 节</a>跟踪学习过程\n",
    "    <br>&nbsp;<br>\n",
    "<a href='#section4'>第 4 节</a> [混合与匹配] 在目标地图或新的陌生场景中部署训练后的代理\n",
    "    <br>&nbsp;<br>\n",
    "<a href='#section5'>第 5 节</a>在 Keras（TensorFlow 后端）构建或修改您自己的深度学习 SC2 代理\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.filterwarnings(\"ignore\") # supress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "# 第 1 节 - SC2 PyGame 客户端与 Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过与 Blizzard 的协同合作，DeepMind 已发布一款高级 Python API (<a href=\"https://github.com/deepmind/pysc2\">pysc2</a>)，我们可以用它来构建与“星际争霸 II (StarCraft II)”引擎进行交互的机器学习代理。\n",
    "\n",
    "下面我们将通过以下导入命令将此库加载到我们的笔记本中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们可以尝试使用 pysc2 来启动一个采用 py-game 渲染客户端的迷你游戏，该客户端将会生成一个低分辨率游戏视图（而我们的深度学习代理在玩游戏时将会用到此视图）。\n",
    "\n",
    "待加载的迷你游戏允许我们使用鼠标和键盘来控制 9 个人族太空<a href=\"http://us.battle.net/sc2/en/game/unit/marine\">陆战队员</a>组成的小队，以此对抗 4 只一组的虫族<a href=\"http://us.battle.net/sc2/en/game/unit/roach\">蟑螂</a>。\n",
    "\n",
    "通过运行以下命令，您将能启动带有“消灭蟑螂 (DefeatRoaches)”地图的 sc2 py-game 客户端。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pysc2.bin.play --map DefeatRoaches --max_game_steps 2000 > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如要查看此地图并在其上游玩，我们需要打开另一个选项卡，该选项卡将会运行一个远程图形桌面连接，而实验运行在AWS 云的实例中。\n",
    "\n",
    "\n",
    "###<center>单击 [noVNC Server](http://ec2-18-221-145-178.us-east-2.compute.amazonaws.com:6900/?password=vncpassword)（noVNC 服务器）查看 Pysc2 客户端。</center>\n",
    "\n",
    "接着，我们来启动游戏客户端，该客户端应会在 VNC 查看器中显示出来。注意，在 600 个游戏步骤结束之后，迷你游戏将会关闭，不过您可以再次重启（通过再次运行单元或继续进行余下内容）。注意：请使用 Internet Explorer 或 Microsoft Edge，因为其他浏览器可能不支持您与游戏交互。\n",
    "\n",
    "现在，您应能选择 VNC 查看器中显示的 GUI 游戏并与之交互。单击并拖动以选择（绿色的陆战队员圈），然后右键单击您想将所选单位移至的目标位置。\n",
    "\n",
    "\n",
    "<img src=\"images/marine_vs_roach.jpg\" width=\"50%\"></img><img src=\"images/marine_vs_roach_in_game.jpg\" width=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您想进一步探索，也可以试试其他迷你游戏，只需将以上单元中的“消灭蟑螂 (DefeatRoaches)”替换成其他的地图名即可。以下列出了一些可供您启动的迷你游戏。\n",
    "\n",
    "*“收集散落水晶 (CollectMineralShards)”\n",
    "*“消灭小狗和毒爆虫 (DefeatZerglingsAndBanelings)”\n",
    "*“寻找并消灭小狗 (FindAndDefeatZerglings)”\n",
    "*“寻路 (MoveToBeacon)”\n",
    "\n",
    "每个迷你游戏都试图教授 AI 一种掌控 SC2 所需的不同技能。代理必须学会什么技能才能掌控“消灭蟑螂 (DefeatRoaches)”？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>常见问题解答：</b>一些浏览器在 VNC 会话中不支持交互式游戏 - 尝试切换浏览器或禁用扩展程序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - 以编程方式探索环境观测结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将学习如何通过编程来创建 SC2 环境并与之交互。我们不直接使用鼠标和键盘，而是使用函数式 API 来探索 SC2 py-game 客户端产生的观测结果，并发出命令。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sc2_RL_environment.png\" style=\"height:350px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# required \n",
    "from absl import flags\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS(['--'])\n",
    "\n",
    "# note this import must happen after flag initialization\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions\n",
    "from pysc2.lib import features\n",
    "\n",
    "# define environment flags\n",
    "env_args = dict(\n",
    "        map_name='DefeatRoaches',\n",
    "        step_mul=1, # How many time steps occur between each action-decision. A step_mul of 1 means an agent can choose one action per frame.\n",
    "        game_steps_per_episode=0, # no limit- but each map has a built-in max number of steps and will terminate after reaching that.\n",
    "        screen_size_px = ( 64, 64 ), \n",
    "        minimap_size_px = ( 32, 32 ),\n",
    "        visualize = True,\n",
    "        score_index = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spawn the environment -- may take a minute to launch\n",
    "env = sc2_env.SC2Env(**env_args) # ** syntax implies variable number named arguments\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Pysc2 可用于 Linux、Windows 或 MacOS 系统。我们将在本实验中使用 Linux 实例，不过您也可下载代理的回放，然后在 Windows 或 Mac 设备上查看。\n",
    "\n",
    "现在，我们可以检阅 sc2 Linux 模拟器在每个游戏步骤后提供的原始观测结果样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=<StepType.FIRST: 0>, reward=0, discount=1.0, observation={'cargo': array([], shape=(0, 7), dtype=int32), 'minimap': array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=int32), 'game_loop': array([1], dtype=int32), 'available_actions': array([  0,   1,   2,   3,   4,   5,   7,  12, 331, 332, 333, 334,  13,\n",
       "       274, 451, 452, 453], dtype=int32), 'screen': array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=int32), 'control_groups': array([[0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0]], dtype=int32), 'multi_select': array([[48,  1, 45,  0,  0,  0,  0],\n",
       "       [48,  1, 45,  0,  0,  0,  0],\n",
       "       [48,  1, 45,  0,  0,  0,  0],\n",
       "       [48,  1, 45,  0,  0,  0,  0],\n",
       "       [48,  1, 45,  0,  0,  0,  0],\n",
       "       [48,  1, 45,  0,  0,  0,  0],\n",
       "       [48,  1, 45,  0,  0,  0,  0],\n",
       "       [48,  1, 45,  0,  0,  0,  0],\n",
       "       [48,  1, 45,  0,  0,  0,  0]], dtype=int32), 'cargo_slots_available': array([0], dtype=int32), 'player': array([1, 0, 0, 9, 0, 9, 0, 0, 9, 0, 0], dtype=int32), 'single_select': array([[0, 0, 0, 0, 0, 0, 0]], dtype=int32), 'build_queue': array([], shape=(0, 7), dtype=int32), 'score_cumulative': array([  0,   0,   0, 450,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "      dtype=int32)})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观测结果中包含众多有趣的数据元素。如果仔细观察，您应能找到当前时间步的奖励信息、累积分数以及许多有关游戏世界现状的其他细节，包括现有动作。由于我们刚刚踏入一个全新的游戏世界，因此这些值中有许多尚无具体信息可供参考；不过，待我们执行一些动作后，稍后就能再次返回查看这些值。下面我们就来演示一下如何获取它们的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('current reward: ', 0)\n",
      "('cumulative score: ', 0)\n",
      "('available data elements: ', ['cargo', 'minimap', 'game_loop', 'available_actions', 'screen', 'control_groups', 'multi_select', 'cargo_slots_available', 'player', 'single_select', 'build_queue', 'score_cumulative'])\n"
     ]
    }
   ],
   "source": [
    "print( 'current reward: ', obs[0].reward )\n",
    "print( 'cumulative score: ', obs[0].observation['score_cumulative'][0])\n",
    "print( 'available data elements: ', obs[0].observation.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们要特别关注一下屏幕的视觉层。\n",
    "\n",
    "这些可见的“特征层”代表各种游戏元素，如单位类型、单位所有者（玩家 1 与玩家 2）、单位生命值以及游戏状态的其他重要方面。每个特征层都作为网络的一个单独输入而提供，但由于这些特征层共用同一个空间参考框架，因此我们可将它们看作不同的通道或维度，共同构成对游戏状态的完整表示（即类似于红、绿、蓝三种颜色通道在照片中的呈现方式）\n",
    "\n",
    "<img src=\"images/feature_layers.png\" style=\"height:350px\">\n",
    "<center>\n",
    "**注意，这是在一场后期战斗中对非常活跃的屏幕进行的可视化。在玩迷你游戏时，我们不会看到如此复杂的场景。*\n",
    "</center>\n",
    "\n",
    "\n",
    "首先，我们将可视化所有数据通道及其名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 17 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numScreenLayers = obs[0].observation['screen'].shape[0] # 17\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "titles = ['heigh_map', 'visibility_map', 'creep', 'power', 'player_id', \n",
    "          'player_relative', 'unit_type', 'selected', \n",
    "          'unit_hit_points', 'unit_hit_points_ratio', \n",
    "          'unit_energy', 'unit_energy_ratio', \n",
    "          'unit_shields', 'unit_shields_ratio', \n",
    "          'unit_density', 'unit_density_aa', 'effects']\n",
    "[ [plt.subplot(5, 4, iScreenLayer+1), plt.imshow( obs[0].observation['screen'][iScreenLayer], aspect='equal'), \n",
    "       plt.title(titles[iScreenLayer]), plt.axis('off')] \n",
    " for iScreenLayer in range(numScreenLayers) ]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以放大其中一个数据通道以便更详细地查看。尝试更改屏幕索引并重新运行以下单元。您可<a href='https://github.com/deepmind/pysc2/blob/master/docs/environment.md#feature-layers'>在以下链接中</a>找到所有层的含义说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "screenIndex = 14 # choose a number between 0 and 16\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow( obs[0].observation['screen'][screenIndex])\n",
    "plt.title(titles[screenIndex])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - 使用 API 发出动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经探索了 sc2 引擎产生的观测结果。下面我们来尝试执行一些可发送至引擎的动作，这样引擎就能作出响应并生成下一个步骤或观测结果。\n",
    "\n",
    "用于在“星际争霸 (StarCraft)”中发出动作的 python 函数式 API 能让我们生成整套动作，供人类玩家通过鼠标和键盘进行交互。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们来看看代理目前可发出哪些动作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/no_op ()\n",
      "1/move_camera (1/minimap [0, 0])\n",
      "2/select_point (6/select_point_act [4]; 0/screen [0, 0])\n",
      "3/select_rect (7/select_add [2]; 0/screen [0, 0]; 2/screen2 [0, 0])\n",
      "4/select_control_group (4/control_group_act [5]; 5/control_group_id [10])\n",
      "5/select_unit (8/select_unit_act [4]; 9/select_unit_id [500])\n",
      "7/select_army (7/select_add [2])\n",
      "12/Attack_screen (3/queued [2]; 0/screen [0, 0])\n",
      "331/Move_screen (3/queued [2]; 0/screen [0, 0])\n",
      "332/Move_minimap (3/queued [2]; 1/minimap [0, 0])\n",
      "333/Patrol_screen (3/queued [2]; 0/screen [0, 0])\n",
      "334/Patrol_minimap (3/queued [2]; 1/minimap [0, 0])\n",
      "13/Attack_minimap (3/queued [2]; 1/minimap [0, 0])\n",
      "274/HoldPosition_quick (3/queued [2])\n",
      "451/Smart_screen (3/queued [2]; 0/screen [0, 0])\n",
      "452/Smart_minimap (3/queued [2]; 1/minimap [0, 0])\n",
      "453/Stop_quick (3/queued [2])\n"
     ]
    }
   ],
   "source": [
    "for iAction in obs[0].observation['available_actions']:\n",
    "    print( actions.FUNCTIONS[iAction] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如要了解如何将一系列鼠标和键盘操作转换为函数式 API 调用，可以参考以下动画图形。\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/deepmind-live-cms-alt/documents/Oriol-Fig-Anim-170809-Optimised-r03.gif\"></img>\n",
    "\n",
    "下方展示了我们可以使用函数式 API 发出的几个示例动作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do nothing\n",
    "doNothing = actions.FunctionCall( 0, [] ) \n",
    "\n",
    "# rectangle select and add to existing selection, rectangle from (0,0) to (31, 31)\n",
    "selectRectangle = actions.FunctionCall(3, [[1], [0,0], [31,31]])\n",
    "\n",
    "# select entire army\n",
    "selectEntireArmy = actions.FunctionCall(7, [[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将选择所有可用的陆军单位，并让它们攻击屏幕上的一个点（同时我们还要为后续 100 个游戏步骤重新发出此命令）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack screen location ( x=60, y=15 ) -- assumes a mini-game with at least 64x64 tiles\n",
    "attackScreen = actions.FunctionCall(12, [[0], [60, 15]])\n",
    "\n",
    "obs = env.step( [ selectEntireArmy ] )\n",
    "for i in range(100):\n",
    "    obs = env.step( [ attackScreen ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - [可选] 动作类型及构建您自己的动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节旨在详述有关 SC2 中编程动作的其他信息。每个动作都有一个特定的结构，这取决于它是否需要修饰符或空间参数。\n",
    "\n",
    "下面列出了几个可能会有的常见动作特征（非详尽列表）：\n",
    "\n",
    "```\n",
    "Type1: action.FunctionCall( functionID )\n",
    "Type2: action.FunctionCall( functionID, [ [ modifier ] ] ) # e.g., 'select_army'\n",
    "Type3: action.FunctionCall( functionID, [ [ modifier ], [x1, y1 ] ) # e.g., 'attack_screen'\n",
    "Type4: action.FunctionCall( functionID, [ [ modifier ], [x1, y1], [x2, y2] ) # e.g., 'select_rect'\n",
    "```\n",
    "<b>Type1</b> 属于简单型动作，例如“Stop_quick”(ID: 453) 或“HoldPosition_quick”(ID: 274)，这些动作都不需要任何修饰符或屏幕坐标。\n",
    "\n",
    "<b>Type2</b> 动作需要一个修饰符 - 在“select_army”(ID: 7) [选择所有存活的陆军单位] 的情况下，修饰符表明应将返回的陆军单位集添加至当前任意选择（例如一个农民单位）还是用其替换现有选择。\n",
    "\n",
    "<b>Type3</b> 动作需要一个修饰符和单个坐标 - 例如“Attack_screen”(ID: 12) 具有修饰符和单个坐标目标，其中修饰符表示是否应立即执行动作（或将动作添加至执行队列）。\n",
    "\n",
    "<b>Type4</b> 动作需要一个修饰符和两个屏幕坐标 - 例如“select_rect”(ID: 3) 需要一个修饰符和两个屏幕坐标，其中修饰符表示是否要替换、连结或从现有选择中删去新选择），屏幕坐标表示定义新选择的边框。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的 RL 代理只能使用此编程接口与游戏世界交互 - 您不妨尝试创建下方列出的有趣的动作序列！\n",
    "\n",
    "<b>实用提示：</b>如有需要，您可以随时将游戏状态<b>重置</b>为开始，具体可通过以下命令```obs = env.reset()```来执行此操作\n",
    "此外，我们还编写了一个 ```safe_action``` 功能（下方显示的几个单元），您可以借助此功能来帮助避免禁止执行的动作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 - 随机动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们已在下方为一个随机选择攻击点的代理编写了代码。您可能会发现，即使是随机代理也能偶然发现良好的行为；事实上，我们最初随机初始化代理参数时，正是据此来自动开启学习过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If the action we chose can't be executed lets try to select our entire army, \n",
    "advance the game state, and try again.\n",
    "\"\"\"\n",
    "\n",
    "def safe_action ( actionToTake, obs ):\n",
    "    if actionToTake.function not in obs.observation['available_actions']:\n",
    "        print('unable to take selected action...lets try to fix things')\n",
    "        print('fix#1: select all army units')\n",
    "        obs = env.step( [ selectEntireArmy ] )\n",
    "        print('fix#2: perform no-op action')\n",
    "        obs = env.step( [ doNothing ] )\n",
    "        if actionToTake.function not in obs[0].observation['available_actions']:\n",
    "            print('!we are really in trouble...consider taking a different action')\n",
    "    else:\n",
    "        obs = env.step( [ actionToTake ] )\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to take selected action...lets try to fix things\n",
      "fix#1: select all army units\n",
      "fix#2: perform no-op action\n",
      "unable to take selected action...lets try to fix things\n",
      "fix#1: select all army units\n",
      "fix#2: perform no-op action\n"
     ]
    }
   ],
   "source": [
    "# obs = env.reset()\n",
    "nCycles = 1000\n",
    "for iCycles in range ( nCycles ):    \n",
    "    randomAttackScreen = actions.FunctionCall( 12, [[0], [np.random.randint(63), np.random.randint(63)]])\n",
    "    obs = safe_action ( randomAttackScreen, obs[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "# 第 2 节 - 奖励塑造"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由 DRL 训练的代理可以根据其训练环境（模拟器）和环境给予它们的奖励来学习行为。我们在相同的环境中训练了两个代理，游戏环境选取的是一张简易的迷你地图，并由两个蟑螂游戏包分离开来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - 通过 SC2 地图编辑器指定奖励\n",
    "我们使用地图编辑器来更改逻辑，以便控制环境所产生的奖励。如果您想构建自己的迷你游戏或修改现有的游戏，请下载并安装可免费游玩的“星际争霸 (StarCraft)”客户端，其中包括地图编辑器。然后，您可以通过修改地形或调整触发器中的逻辑来编辑地图。下方展示了默认设置的“消灭蟑螂 (DefeatRoaches)”地图的逻辑屏幕截图，其中每杀死一只蟑螂，都会增加 10 分（仅供参考：玩家一是我们的 RL 代理，玩家二是游戏中的脚本化 AI）。\n",
    "\n",
    "<img src=\"images/map_editor.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - 奖励修改和紧急行为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们利用 SC2 地图编辑器，构建了几种不同的“消灭蟑螂 (DefeatRoaches)”地图。在一个实例中，每流失一秒，我们就会施加惩罚，以此鼓励代理主动出击。经过长时间训练后，请执行以下单元查看此代理的记录。注意，上文中的代理一直在四处移动以寻找要击杀的敌人。通过更快速地发现及击杀所有蟑螂，此代理可获得更高的分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/8eRFzXtBdwA?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/8eRFzXtBdwA?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们制作了一张地图，其将针对小队成员的损失施加更严厉的惩罚。请执行以下单元查看这个不善主动出击的代理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9Opgktl6kLo?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9Opgktl6kLo?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那位好斗的代理一直在四处寻找要击杀的蟑螂，而这位保守的代理却更乐意在角落里闲逛，以免损失任何陆战队员。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "# 第 3 节 - 跟踪学习过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过查看代理所能获得的奖励，我们能够加深对它的了解。通过绘制这些训练时间或所见帧数的曲线，我们便能得到一条奖励曲线。我们使用 Tensorflow 的 Tensorboard 绘制了曲线。您可以在此 [链接](/tensorboard/) 中检阅许多不同代理的曲线。\n",
    "\n",
    "单击“toggle all runs”（触发所有运行进程），然后单击您想要探索的运行进程。稍后我们便会捕捉到许多指标，且 sc2 或 episode_score 会获得环境给予代理的分数。\n",
    "\n",
    "我们使用此开源 repo 来训练代理：https://github.com/simonmeister/pysc2-rl-agents.git 下图展示了在“消灭蟑螂 (DefeatRoaches)”迷你游戏中训练的代理的奖励曲线。\n",
    "<center>\n",
    "<img src=\"images/DefeatRoaches.PNG\" width=\"75%\"></img>\n",
    "</center>\n",
    "您可从学习曲线中看到，代理通常必须牺牲自己的分数方能学习一种新策略，最终取得更出色的游戏表现。下方提供了两个视频。第一个是只训练了一半的代理，第二个是完整训练后的代理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - 只训练了一半的代理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tzPrtTXPTEA?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tzPrtTXPTEA?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，代理已经学会将注意力集中在蟑螂身上。不过，它仍在学习，并会花些时间点击地图的各个点。\n",
    "\n",
    "训练结束时，此代理效率惊人，并能轻松聚焦在蟑螂身上。运气好的话，代理将能斩获超高分！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IBUgp6097Q0?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IBUgp6097Q0?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - 多个环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让代理同时在多个环境中运行可以加快其学习速度。我们使用 16 个环境同时训练其中一个代理，而对另一个代理则只使用一个环境。\n",
    "\n",
    "\n",
    "<img src=\"images/MoveToBeaconNenvs.PNG\" width=\"75%\"></img>\n",
    "在 16 个环境中训练的代理能够在 20 分钟内掌握“寻路 (MoveToBeacon)”迷你地图的游戏技能，而只使用 1 个环境训练的代理则需要 2 小时 20 分钟。\n",
    "\n",
    "“寻路 (MoveToBeacon)”是一个十分简单的迷你游戏，因此只在一个环境中学习的代理仍然可以掌握它。对于更复杂的环境而言，代理通常需要在多个环境中进行训练，以便学习掌握游戏所需的高级策略。\n",
    "\n",
    "浏览 [TensorBoard](/tensorboard/)，进一步探索奖励曲线。TensorBoard 也有一条为同时在 8 个环境中训练的代理绘制的奖励曲线。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "# 第 4 节 - 混合与匹配代理和环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本节中，您可以选择一个代理，然后让该代理玩一个迷你游戏。您可以自由试验，探索不同的代理在不同环境下会有何表现。下方展示了一个可用代理列表以及可将其部署到的环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可用代理\n",
    "\n",
    "* <b>DefeatRoaches</b> - 在“消灭蟑螂 (DefeatRoaches)”地图上受过完整训练的代理\n",
    "* <b>DefeatRoaches_half_trained</b> - 在“消灭蟑螂 (DefeatRoaches)”地图上受过部分训练的代理\n",
    "* <b>DefeatRoaches_singleRoundReset_conserveMarines_noTimePenalty_splitRoachPacks</b> - 竭力避免损失单位的代理\n",
    "* <b>DefeatRoaches_singleRoundReset_highTimePenalty_splitRoachPacks</b> - 必须尽快杀死对手的代理\n",
    "* <b>MoveToBeacon_n_envs_16</b> - 训练进行寻路的代理\n",
    "\n",
    "### 环境 [<a href=\"https://github.com/deepmind/pysc2/blob/master/docs/mini_games.md\">详细描述</a>]\n",
    "<ul>\n",
    "“训练陆战队员 (BuildMarines)”<b>||</b>“收集散落水晶 (CollectMineralShards)”<b>||</b>“收集水晶和气 (CollectMineralsAndGas)”<br >\n",
    "“消灭蟑螂 (DefeatRoaches)”<b>||</b>“消灭小狗和毒爆虫 (DefeatZerglingsAndBanelings)”<b>||</b>“寻找并消灭小狗 (FindAndDefeatZerglings)”<br >\n",
    "“寻路 (MoveToBeacon)”\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如要加载代理，我们必须指定地图和检查点。输入下方的代理名称以查看模型检查点编号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\t\t\t       model.ckpt-250000.index\r\n",
      "model.ckpt-250000.data-00000-of-00001  model.ckpt-250000.meta\r\n"
     ]
    }
   ],
   "source": [
    "!ls /notebooks/models/DefeatRoaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“检查点”文件会指向要加载的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"model_checkpoint_path: \\\"/notebooks/models/DefeatRoaches/model.ckpt-250000\\\"\" > /notebooks/models/DefeatRoaches/checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在完成 10 次训练之后，回放将被写入磁盘。\n",
    "\n",
    "完成后，请单击工具栏上的 ```Stop```（停止）按钮或前往 ```Kernel```（内核）菜单，然后选择 ```Interrupt ```（中断），这样您便可执行更多单元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "2018-11-23 01:19:41.864234: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "Version: B55958 (SC2.3.16)\n",
      "Build: Jul 31 2017 13:19:41\n",
      "Command Line: '\"/headless/StarCraftII/Versions/Base55958/SC2_x64\" -listen 127.0.0.1 -port 24657 -dataDir /headless/StarCraftII/ -tempDir /tmp/sc-NLrHsV/ -displayMode 0'\n",
      "Starting up...\n",
      "Startup Phase 1 complete\n",
      "2018-11-23 01:19:41.941499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-11-23 01:19:41.941854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:1e.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-11-23 01:19:41.941903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\n",
      "Startup Phase 2 complete\n",
      "Creating stub renderer...\n",
      "Listening on: 127.0.0.1:24657 (24657)\n",
      "Startup Phase 3 complete. Ready for commands.\n",
      "Requesting to join a single player game\n",
      "Configuring interface options\n",
      "Configure: raw interface enabled\n",
      "Configure: feature layer interface enabled\n",
      "Configure: score interface enabled\n",
      "Configure: render interface disabled\n",
      "Entering load game phase.\n",
      "Launching next game.\n",
      "Next launch phase started: 2\n",
      "Next launch phase started: 3\n",
      "Next launch phase started: 4\n",
      "Next launch phase started: 5\n",
      "Next launch phase started: 6\n",
      "Next launch phase started: 7\n",
      "Next launch phase started: 8\n",
      "Game has started.\n",
      "Sending ResponseJoinGame\n",
      "ALSA lib confmisc.c:768:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:4292:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\n",
      "ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:4292:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1251:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:4292:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:4771:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2266:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "Loaded agent at train_step 250000\n",
      "episode 0: score = 56.000000\n",
      "Game has started.\n",
      "episode 1: score = 46.000000\n",
      "Game has started.\n",
      "episode 2: score = 36.000000\n",
      "Game has started.\n",
      "episode 3: score = 46.000000\n",
      "Game has started.\n",
      "episode 4: score = 11.000000\n",
      "Game has started.\n",
      "episode 5: score = 36.000000\n",
      "Game has started.\n",
      "episode 6: score = 46.000000\n",
      "Game has started.\n",
      "episode 7: score = 56.000000\n",
      "Game has started.\n",
      "episode 8: score = 56.000000\n",
      "Game has started.\n",
      "episode 9: score = 21.000000\n",
      "Game has started.\n",
      "episode 10: score = 46.000000\n",
      "Game has started.\n",
      "episode 11: score = 141.000000\n",
      "Game has started.\n",
      "episode 12: score = 56.000000\n",
      "Game has started.\n",
      "episode 13: score = 46.000000\n",
      "Game has started.\n",
      "episode 14: score = 312.000000\n",
      "Game has started.\n",
      "episode 15: score = 36.000000\n",
      "Game has started.\n",
      "episode 16: score = 81.000000\n",
      "Game has started.\n",
      "episode 17: score = 26.000000\n",
      "Game has started.\n",
      "episode 18: score = 46.000000\n",
      "Game has started.\n",
      "episode 19: score = 81.000000\n",
      "Game has started.\n",
      "episode 20: score = 46.000000\n",
      "Game has started.\n",
      "episode 21: score = 46.000000\n",
      "Game has started.\n",
      "episode 22: score = 36.000000\n",
      "Game has started.\n",
      "episode 23: score = 70.000000\n",
      "Game has started.\n",
      "episode 24: score = 81.000000\n",
      "Game has started.\n",
      "episode 25: score = 46.000000\n",
      "Game has started.\n",
      "episode 26: score = 46.000000\n",
      "Game has started.\n",
      "episode 27: score = 71.000000\n",
      "Game has started.\n",
      "episode 28: score = 71.000000\n",
      "Game has started.\n",
      "episode 29: score = 46.000000\n",
      "Game has started.\n",
      "episode 30: score = 106.000000\n",
      "Game has started.\n",
      "episode 31: score = 26.000000\n",
      "Game has started.\n",
      "episode 32: score = 56.000000\n",
      "Game has started.\n",
      "^C\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/notebooks/pysc2-rl-agents/rl/environment.py\", line 50, in worker\n",
      "    timesteps = env.step([action])\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pysc2/lib/stopwatch.py\", line 197, in _stopwatch\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pysc2/env/sc2_env.py\", line 257, in step\n",
      "    return self._step()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pysc2/env/sc2_env.py\", line 261, in _step\n",
      "    self._obs = self._parallel.run(c.observe for c in self._controllers)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pysc2/lib/run_parallel.py\", line 56, in run\n",
      "    return [funcs[0]()]\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pysc2/lib/remote_controller.py\", line 80, in _valid_status\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pysc2/lib/stopwatch.py\", line 197, in _stopwatch\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pysc2/lib/remote_controller.py\", line 157, in observe\n",
      "    return self._client.send(observation=sc_pb.RequestObservation())\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pysc2/lib/protocol.py\", line 127, in send\n",
      "    res = self.send_req(sc_pb.Request(**kwargs))\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pysc2/lib/protocol.py\", line 113, in send_req\n",
      "    return self.read()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pysc2/lib/stopwatch.py\", line 197, in _stopwatch\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pysc2/lib/protocol.py\", line 86, in read\n",
      "    response = self._read()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pysc2/lib/protocol.py\", line 152, in _read\n",
      "    response_str = self._sock.recv()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/websocket/_core.py\", line 300, in recv\n",
      "    opcode, data = self.recv_data()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/websocket/_core.py\", line 317, in recv_data\n",
      "    opcode, frame = self.recv_data_frame(control_frame)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/websocket/_core.py\", line 330, in recv_data_frame\n",
      "    frame = self.recv_frame()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/websocket/_core.py\", line 364, in recv_frame\n",
      "    return self.frame_buffer.recv_frame()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/websocket/_abnf.py\", line 361, in recv_frame\n",
      "    self.recv_header()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/websocket/_abnf.py\", line 309, in recv_header\n",
      "    header = self.recv_strict(2)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/websocket/_abnf.py\", line 396, in recv_strict\n",
      "    bytes_ = self.recv(min(16384, shortage))\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/websocket/_core.py\", line 434, in _recv\n",
      "    return recv(self.sock, bufsize)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/websocket/_socket.py\", line 81, in recv\n",
      "    bytes_ = sock.recv(bufsize)\n",
      "KeyboardInterrupt\n",
      "mean score: 61.454545\n"
     ]
    }
   ],
   "source": [
    "!python /notebooks/pysc2-rl-agents/run.py \\\n",
    "  DefeatRoaches \\\n",
    "  --map DefeatRoaches \\\n",
    "  --max_windows 1 --gpu 0 --envs 1 \\\n",
    "  --step_mul 8 --steps_per_batch 16 \\\n",
    "  --vis --eval \\\n",
    "  --save_dir /notebooks/models \\\n",
    "  --summary_dir /notebooks/summary \\\n",
    "  --iters 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当生成回放后，您可以执行以下单元压缩文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: notebooks/replays/DefeatRoaches_2018-11-23-01-20-09.SC2Replay (deflated 15%)\r\n",
      "  adding: notebooks/replays/DefeatRoaches_2018-11-23-01-20-30.SC2Replay (deflated 9%)\r\n",
      "  adding: notebooks/replays/DefeatRoaches_2018-11-23-01-20-48.SC2Replay (deflated 7%)\r\n"
     ]
    }
   ],
   "source": [
    "!apt-get update > /dev/null 2>&1 && apt-get install zip > /dev/null 2>&1 && zip /notebooks/replays.zip /notebooks/replays/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[单击此处下载回放。](replays.zip)\n",
    "\n",
    "如要使用本地客户端查看回放，请将回放文件置于 ```~/StarCraftII/replays```，并将迷你游戏置于 ```~/StarCraftII/Maps/mini_games``` 中。您可以在 [此处](https://github.com/deepmind/pysc2/tree/master/pysc2/maps/mini_games) 下载迷你游戏。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "# 第 5 节 - 构建并训练自己的代理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们希望您喜欢本实验，也希望您能学到一些关于 RL 和 SC2 的新知识。我们只是介绍了一些浅显知识，如果您有意深究，我们将乐意提供一些工具来进行更多探索。\n",
    "\n",
    "我们使用基于 TensorFlow 后端的 Keras 为一个自定义代理构建了训练工具，如下所示。我们的目标是为任何有意在 SC2 环境中试验 RL 想法的探究者提供一个易于使用的测试环境。\n",
    "\n",
    "此代码工具的结构如下：\n",
    "<ul>\n",
    "1 - 定义关键参数<br>\n",
    "2 - 创建多个 SC2 环境<br>\n",
    "3 - 加载代理代码<br>\n",
    "4 - 运行交互循环（使用 n-step 轨迹）并更新代理参数<br>\n",
    "</ul>\n",
    "从技术角度来说，该训练工具使用 n-step TD 进行学习，并可将观测的帧序列叠加在一起以探究动态变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require([\"base/js/dialog\"], function(dialog) {dialog.modal({title: 'Cleaning Up', body: 'We need to ensure that no other sc2 environments are active -- lets force a kernel restart', buttons: {'Kernel restart': { click: function(){ Jupyter.notebook.session.restart(); } }}});});\n",
       "Jupyter.notebook.session.delete();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "require([\"base/js/dialog\"], function(dialog) {dialog.modal({title: 'Cleaning Up', body: 'We need to ensure that no other sc2 environments are active -- lets force a kernel restart', buttons: {'Kernel restart': { click: function(){ Jupyter.notebook.session.restart(); } }}});});\n",
    "Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import os.path\n",
    "\n",
    "# required \n",
    "from absl import flags\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS(['initialize FLAGS for sc2 environments'])\n",
    "\n",
    "# note this import must happen after flag initialization\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions\n",
    "from pysc2.lib import features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 - 定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "envParams = { \n",
    "    'simultaneousEnvironments': 4,\n",
    "    'nEnvironmentsToVisualize': 4,\n",
    "    'nTrajectorySteps': 16,\n",
    "    'nStackedFrames': 4,\n",
    "    'agentStepsPerEnvironmentStep': 4,\n",
    "    \n",
    "    'batchCheckpointInterval': 100,\n",
    "    'experimentDirectory': '/experiments',\n",
    "    'debugFlag': False,\n",
    "    \n",
    "    'screenResX': 64,\n",
    "    'screenResY': 64,\n",
    "    'screenResX_minimap': 32,\n",
    "    'screenResY_minimap': 32,\n",
    "    \n",
    "    'screenChannelsToKeep': [ 4, 5, 6, 7, 8, 9 ], # player_ID, player_relative, unit_type, selected, unit_hit_points, unit_hit_points_ratio\n",
    "    'screenChannelsRetained': 6,\n",
    "\n",
    "    'nonSpatialInputDimensions': 12, \n",
    "    'allowedActionIDs': [ 3, 12 ], # select_rect, attack_screen\n",
    "    'allowedActionIDRequiresModifier': [ 2, 1 ],            \n",
    "    'allowedActionIDRequiresLocation': [ 2, 1 ],       \n",
    "\n",
    "    'prunedActionSpaceSize': 2,\n",
    "    'actionArgumentSize': 4, \n",
    "\n",
    "    'nonVisualInputLength': 13,\n",
    "    \n",
    "    'futureDiscountRate': 1,   \n",
    "    'stepTypeFirst': 0,\n",
    "    'stepTypeMid': 1,\n",
    "    'stepTypeLast': 2,\n",
    "    \n",
    "    'entropyWeight': .25,\n",
    "    'policyWeight': 1,\n",
    "    'valueWeight': .5,\n",
    "    \n",
    "}\n",
    "\n",
    "# sanity check environment parameter definition\n",
    "assert ( envParams['prunedActionSpaceSize'] == len(envParams['allowedActionIDs']) \\\n",
    "            == len(envParams['allowedActionIDRequiresModifier']) \\\n",
    "            == len(envParams['allowedActionIDRequiresLocation']) )\n",
    "assert ( envParams['screenChannelsRetained'] == len(envParams['screenChannelsToKeep'] ) )\n",
    "\n",
    "\n",
    "assert ( envParams['nStackedFrames'] <= envParams['nTrajectorySteps'])\n",
    "\n",
    "sc2EnvLaunchParams = {\n",
    "    'map_name':'DefeatRoaches',\n",
    "    'step_mul': envParams['agentStepsPerEnvironmentStep'],\n",
    "    'game_steps_per_episode': 0, # no limit\n",
    "    'screen_size_px': ( envParams['screenResX'], envParams['screenResX']), \n",
    "    'minimap_size_px': ( envParams['screenResX_minimap'], envParams['screenResY_minimap']),\n",
    "    'visualize': False,\n",
    "    'score_index': None \n",
    "}\n",
    "sc2EnvLaunchParamsVis = sc2EnvLaunchParams.copy()\n",
    "sc2EnvLaunchParamsVis['visualize'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 - 启动多个环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将派生多个并行的工作进程，每个进程都自带内存空间且会运行一个 sc2 linux 客户端实例。每个工作进程都将通过一个双向通信管道连接到当前进程（笔记本内核），我们可通过此管道发送请求并与其中的 sc2 环境进行交互。\n",
    "\n",
    "<img src=\"images/multi_process.PNG\" style=\"width:40%\">\n",
    "\n",
    "对于每个管道而言，连接至主进程的末端称为局部末端，而连接至工作进程的末端则称为远程末端。我们使用管道末端在进程之间进行通信，通信时使用 ```.send()``` 命令进行传输并使用 ```.recv()``` 命令来监听回应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Pipe\n",
    "import time\n",
    "\n",
    "# define the function running in each [forked] worker to parse communications between itself and the main process\n",
    "def sc2_remote_env_manager ( workerID, remotePipeEnd, sc2EnvLaunchParams ):\n",
    "    print('starting remote sc2 environment thread# ' + str(workerID))\n",
    "    env = sc2_env.SC2Env(**sc2EnvLaunchParams)\n",
    "    obs = env.reset()    \n",
    "    remotePipeEnd.send( ( 'launch complete', obs[0]) )\n",
    "    \n",
    "    while True:\n",
    "        command, arguments = remotePipeEnd.recv()\n",
    "        # take action and advance the game environment \n",
    "        if command == 'step': \n",
    "            obs = env.step( [ arguments ] )\n",
    "            assert( len(obs) == 1 )\n",
    "            remotePipeEnd.send( obs[0] )\n",
    "        elif command == 'reset':\n",
    "            obs = env.reset()    \n",
    "            remotePipeEnd.send( obs[0] )\n",
    "        # close the pipe and sc2 environment\n",
    "        elif command == 'close':\n",
    "            remotePipeEnd.send('closing')\n",
    "            remotePipeEnd.close()\n",
    "            break\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "localPipeEnds = [] # pipe-ends/communication channels used by the main process to communicate with workers\n",
    "remotePipeEnds = [] # pipe-ends/communication channels used by the workers to communicate with the main process\n",
    "processList = [] # list of worker threads\n",
    "\n",
    "# create two-way-communication channels [aka pipes] for remote workers and our main program\n",
    "# and spawn remote processes with remote pipe ends as an input argument\n",
    "for iEnv in range(envParams['simultaneousEnvironments']):\n",
    "    # create new pipe\n",
    "    localPipeEnd, remotePipeEnd = Pipe()\n",
    "    # store both ends \n",
    "    localPipeEnds += [ localPipeEnd ]\n",
    "    remotePipeEnds += [ remotePipeEnd ]\n",
    "    # spawn remote process and connect to remote pipe end\n",
    "    if iEnv < envParams['nEnvironmentsToVisualize']:\n",
    "        processList += [ Process( target = sc2_remote_env_manager , args = ( iEnv, remotePipeEnd, sc2EnvLaunchParamsVis) ) ]\n",
    "    else:        \n",
    "        processList += [ Process( target = sc2_remote_env_manager , args = ( iEnv, remotePipeEnd, sc2EnvLaunchParams) ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = {}\n",
    "for iEnv in range ( envParams['simultaneousEnvironments'] ):\n",
    "    obs[iEnv] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting remote sc2 environment thread# 0\n",
      "launch complete\n",
      "starting remote sc2 environment thread# 1\n",
      "launch complete\n",
      "starting remote sc2 environment thread# 2\n",
      "launch complete\n",
      "starting remote sc2 environment thread# 3\n",
      "launch complete\n"
     ]
    }
   ],
   "source": [
    "# start remote workers, wait for each process to fully bring up the sc2 environment before creating the next\n",
    "for iEnv in range(envParams['simultaneousEnvironments']):\n",
    "    processList[iEnv].start()\n",
    "    while not localPipeEnds[iEnv].poll():\n",
    "        time.sleep(1)\n",
    "    msg, obs[iEnv] = localPipeEnds[iEnv].recv()\n",
    "    print ( msg )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>单击 [noVNC Server](http://ec2-18-221-145-178.us-east-2.compute.amazonaws.com:6900/?password=vncpassword)（noVNC 服务器）查看环境。</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 - 加载代理代码\n",
    "\n",
    "以下显示了 [一个卷积代理的代码](../edit/convolutional_agent.py) - 您可以随时试验此代码并作出改进！\n",
    "此代理由基础版 [sc2_agent](../edit/convolutional_agent.py) 衍生而来 - 您可将此代理视作骨架，将卷积代理视作实际进行特征提取的大脑。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "''' if using python3 uncomment -> ''' # from importlib import reload \n",
    "import convolutional_agent as sc2RL\n",
    "reload(sc2RL);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi I'm the CONV-AGENT\n",
      " | architecture: im a simple conv agent\n",
      " | learning strategy: backprop\n"
     ]
    }
   ],
   "source": [
    "agent = sc2RL.ConvAgent( envParams )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 24, 64, 64)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 64, 64)   12320       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 64, 64)   16416       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 48)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 131072)       0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 131120)       0           input_2[0][0]                    \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          67133952    concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          67133952    concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          262656      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 512)          262656      dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          262656      dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 512)          262656      dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 1, 64, 64)    289         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 1, 64, 64)    289         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1024)         0           dense_3[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4096)         0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4096)         0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            1025        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 2)            2050        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            1025        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1)            1025        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "softmax_1 (Softmax)             (None, 4096)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_2 (Softmax)             (None, 4096)         0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 8197)         0           dense_7[0][0]                    \n",
      "                                                                 dense_8[0][0]                    \n",
      "                                                                 dense_9[0][0]                    \n",
      "                                                                 dense_10[0][0]                   \n",
      "                                                                 softmax_1[0][0]                  \n",
      "                                                                 softmax_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 135,352,967\n",
      "Trainable params: 135,352,967\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.model_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset environments\n",
    "for iEnv in range( envParams['simultaneousEnvironments'] ):\n",
    "    localPipeEnds[iEnv].send ( ( 'reset', [] ) )\n",
    "    obs[iEnv] = localPipeEnds[iEnv].recv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 - 行动 - 观察 - 学习 [循环]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/RL_diagram.PNG\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectory# 15 avg step reward: 0.0\n",
      "trajectory# 16 avg step reward: 0.0\n",
      "trajectory# 17 avg step reward: -0.014705882\n",
      "trajectory# 18 avg step reward: 0.11764706\n",
      "trajectory# 19 avg step reward: 0.11764706\n",
      "trajectory# 20 avg step reward: -0.014705882\n",
      "trajectory# 21 avg step reward: -0.029411765\n",
      "trajectory# 22 avg step reward: -0.014705882\n",
      "trajectory# 23 avg step reward: 0.0\n",
      "trajectory# 24 avg step reward: 0.11764706\n",
      "trajectory# 25 avg step reward: 0.11764706\n",
      "trajectory# 26 avg step reward: 0.14705883\n",
      "trajectory# 27 avg step reward: 0.13235295\n",
      "trajectory# 28 avg step reward: 0.0\n",
      "trajectory# 29 avg step reward: -0.04411765\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "trainBatches = 15 # change to a large number or replace first for loop with a 'while True:' loop\n",
    "\n",
    "for iTrajectory in range(trainBatches):\n",
    "    for iStep in range ( envParams['nTrajectorySteps'] + 1):        \n",
    "        ''' batch predict model outputs on current inputs ''' # [ 1 timestep in multiple environments ]\n",
    "        batchModelOutputs = agent.batch_predict ( np.squeeze( agent.nEnvOneStepBatch, axis=1),\n",
    "                                                      np.squeeze( agent.nEnvOneStepBatchNonSpatial, axis=1)  )\n",
    "        \n",
    "        ''' update trajectory rewards and value estimates '''\n",
    "        agent.rewards[:, iStep ] = [ obs[iEnv].reward for iEnv in list(obs.keys()) ]\n",
    "        agent.valuePredictions[:, iStep] = batchModelOutputs[:, agent.policyInds['value']]\n",
    "        \n",
    "        if iStep != envParams['nTrajectorySteps']: # don't advance when in the final step -- use it to bootstrap loss computation\n",
    "            ''' sample and mask '''\n",
    "            sc2functionCalls, actionIDs, actionArguments = \\\n",
    "                agent.sample_and_mask ( obs, batchModelOutputs )\n",
    "\n",
    "            ''' compute partial loss terms ''' # logProbs and masked policy entropy\n",
    "            agent.inplace_update_logProbs_and_entropy ( iStep, batchModelOutputs )\n",
    "\n",
    "            ''' step i.e. apply selected action in each environment ''' # and get new observations\n",
    "            obs, _ = agent.step_in_envs ( obs, localPipeEnds, sc2functionCalls, actionIDs )\n",
    "\n",
    "            ''' compile the spatial and non-spatial trajectory observations ''' # needed for batch train update\n",
    "            agent.inplace_update_trajectory_observations( iStep, obs )\n",
    "        \n",
    "    ''' finished generating a trajectory -- compute nStep returns, advantages, and cumulative loss '''\n",
    "    agent.compute_loss ()\n",
    "    agent.train()\n",
    "    \n",
    "    print( 'trajectory# ' + str(trainBatches) + ' avg step reward: ' + str(np.mean( np.mean( agent.rewards ))))\n",
    "    if (trainBatches + 1) % envParams['batchCheckpointInterval'] == 0:\n",
    "        agent.model_checkpoint()            \n",
    "    trainBatches += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结束语\n",
    "\n",
    "感谢您参与本实验。如果您想进一步探索强化学习的缤纷世界，请随时探索以下资源：\n",
    "\n",
    "Udacity 深度学习强化课程\n",
    "https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893\n",
    "\n",
    "DeepMind 对深度学习强化方法的概述\n",
    "https://deepmind.com/blog/deep-reinforcement-learning/\n",
    "\n",
    "探讨策略梯度网络的博文\n",
    "https://medium.com/@gabogarza/deep-reinforcement-learning-policy-gradients-8f6df70404e6\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
